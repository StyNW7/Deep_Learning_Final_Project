{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pK3-C-NOpYcI",
        "outputId": "7b273ae7-1658-458e-bf0f-327afc67a64c"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bappekim/air-pollution-in-seoul\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyD_LrYopMyg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plfdcQ59pQTy"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "SEQ_LEN = 24\n",
        "NUM_NODES = 13\n",
        "NUM_FEATURES = 10\n",
        "TARGET_DIM = 6 #New: changed to 6 from 1\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 10\n",
        "\n",
        "# New: Define exactly which columns we want to predict (indices 0 to 5)\n",
        "TARGET_COLS = ['NO2', 'O3', 'CO', 'SO2', 'PM10', 'PM2.5']\n",
        "target_indices = [0, 1, 2, 3, 4, 5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-le3BY1YpRty"
      },
      "outputs": [],
      "source": [
        "def preprocess_seoul_data(df, scalers, look_back_window=24):\n",
        "    # Ensure Date format\n",
        "    df['Measurement date'] = pd.to_datetime(df['Measurement date'])\n",
        "\n",
        "    # CRITICAL FIX: Sort by Date AND Station Code to ensure alignment\n",
        "    df = df.sort_values(by=['Measurement date', 'Station code'])\n",
        "\n",
        "    # Time Encoding\n",
        "    df['hour'] = df['Measurement date'].dt.hour\n",
        "    df['month'] = df['Measurement date'].dt.month\n",
        "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
        "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
        "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12.0)\n",
        "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12.0)\n",
        "\n",
        "    # Select Features\n",
        "    feature_cols = ['NO2', 'O3', 'CO', 'SO2', 'PM10', 'PM2.5',\n",
        "                    'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
        "\n",
        "    # Get sorted list of stations to ensure consistency\n",
        "    stations = sorted(df['Station code'].unique())\n",
        "    num_stations = len(stations)\n",
        "\n",
        "    processed_features = []\n",
        "\n",
        "    for feat in feature_cols:\n",
        "        values = df[feat].values.reshape(-1, num_stations)\n",
        "\n",
        "        # Interpolate missing values\n",
        "        val_df = pd.DataFrame(values)\n",
        "        val_df = val_df.interpolate(method='linear', limit_direction='both')\n",
        "        # val_df = val_df.fillna(method='bfill').fillna(method='ffill') # Extra safety\n",
        "        val_df = val_df.bfill().ffill()\n",
        "\n",
        "        # Normalize only physical pollutants, not sin/cos\n",
        "        if 'sin' not in feat and 'cos' not in feat:\n",
        "            # scaler = MinMaxScaler()\n",
        "            # values_norm = scaler.fit_transform(val_df.values)\n",
        "            scaler_feat = MinMaxScaler()\n",
        "            values_norm = scaler_feat.fit_transform(val_df.values)\n",
        "            scalers[feat] = scaler_feat\n",
        "        else:\n",
        "            values_norm = val_df.values\n",
        "\n",
        "        processed_features.append(values_norm)\n",
        "\n",
        "    # Stack features: (Time, Nodes, Features)\n",
        "    data_block = np.stack(processed_features, axis=-1)\n",
        "\n",
        "    X, Y = [], []\n",
        "    total_time_steps = data_block.shape[0]\n",
        "\n",
        "    #new\n",
        "    # target_idx = 5 # PM2.5\n",
        "    # INDICES for: ['NO2', 'O3', 'CO', 'SO2', 'PM10', 'PM2.5']\n",
        "    target_indices = [0, 1, 2, 3, 4, 5]\n",
        "\n",
        "    for i in range(total_time_steps - look_back_window - 1):\n",
        "        X.append(data_block[i : i + look_back_window, :, :])\n",
        "        # Y.append(data_block[i + look_back_window, :, target_idx])\n",
        "        # CHANGED: Grab all target columns, not just index 5\n",
        "        Y.append(data_block[i + look_back_window, :, target_indices])\n",
        "\n",
        "    # return torch.FloatTensor(np.array(X)), torch.FloatTensor(np.array(Y)).unsqueeze(-1)\n",
        "    # CHANGED: Removed .unsqueeze(-1) because Y is already shape (Batch, Nodes, 6)\n",
        "    return torch.FloatTensor(np.array(X)), torch.FloatTensor(np.array(Y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIw6bbDkpeI_",
        "outputId": "e3cac2d0-ebdd-46fd-e633-550e97352157"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(path + '/AirPollutionSeoul/Measurement_summary.csv')\n",
        "df_station = pd.read_csv(path + '/AirPollutionSeoul/Original Data/Measurement_station_info.csv')\n",
        "\n",
        "# Filter Stations\n",
        "keep_stations = [101, 102, 105, 106, 107, 109, 111, 112, 113, 119, 120, 121, 122]\n",
        "df = df[df['Station code'].isin(keep_stations)]\n",
        "df_station = df_station[df_station['Station code'].isin(keep_stations)]\n",
        "\n",
        "# CRITICAL FIX: Sort station info so the graph matches the data tensors\n",
        "df_station = df_station.sort_values('Station code')\n",
        "\n",
        "# Generate Tensors\n",
        "scalers = {}\n",
        "train_x, train_y = preprocess_seoul_data(df, scalers, SEQ_LEN)\n",
        "print(f\"Data Prepared: Input {train_x.shape}, Target {train_y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjmHv4vc0goK",
        "outputId": "0a9edf02-6607-4be0-a88a-9fb2a185b3d1"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "joblib.dump(scalers, \"seoul_scalers.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S9rejlS60Ka"
      },
      "outputs": [],
      "source": [
        "def get_adjacency_matrix_dynamic(lat_lon_df, threshold_percentile=0.5):\n",
        "    \"\"\"\n",
        "    1. Calculates pairwise distances.\n",
        "    2. Dynamically sets sigma based on the standard deviation of distances.\n",
        "    3. Generates the Gaussian kernel graph.\n",
        "    \"\"\"\n",
        "    # 1. Calculate Pairwise Distances (Euclidean for Lat/Lon)\n",
        "    coords = lat_lon_df[['Latitude', 'Longitude']].values\n",
        "    dists = cdist(coords, coords, metric='euclidean')\n",
        "\n",
        "    # 2. Dynamic Sigma Calculation\n",
        "    # We flatten the distance matrix and remove 0s (self-loops)\n",
        "    non_zero_dists = dists[dists > 0]\n",
        "\n",
        "    # Heuristic: Sigma = Standard Deviation of all distances\n",
        "    # This automatically scales to the unit (Degrees vs Meters)\n",
        "    sigma = np.std(non_zero_dists)\n",
        "\n",
        "    print(f\"--- Dynamic Graph Statistics ---\")\n",
        "    print(f\"Calculated Sigma (Std Dev): {sigma:.6f}\")\n",
        "\n",
        "    # 3. Build Weight Matrix\n",
        "    num_nodes = len(lat_lon_df)\n",
        "    adj = np.zeros((num_nodes, num_nodes))\n",
        "\n",
        "    for i in range(num_nodes):\n",
        "        for j in range(num_nodes):\n",
        "            if i == j:\n",
        "                adj[i][j] = 1.0\n",
        "            else:\n",
        "                d = dists[i][j]\n",
        "                # Gaussian Kernel\n",
        "                weight = np.exp(- (d**2) / (sigma**2))\n",
        "                adj[i][j] = weight\n",
        "\n",
        "    # 4. Thresholding (Optional but Recommended)\n",
        "    # Filter out weak connections to prevent the \"Everything Connected\" problem\n",
        "    # We keep only edges stronger than the 'threshold_percentile' of weights\n",
        "    # But usually, just checking if weight > epsilon (e.g. 0.1) is enough.\n",
        "\n",
        "    weight_threshold = 0.1 # Edges weaker than this are cut\n",
        "    adj[adj < weight_threshold] = 0.0\n",
        "\n",
        "    # Check Connectivity\n",
        "    num_edges = np.count_nonzero(adj) - num_nodes # subtract self-loops\n",
        "    density = num_edges / (num_nodes * (num_nodes - 1))\n",
        "    print(f\"Graph Density: {density:.2%} (Target: 20-50%)\")\n",
        "\n",
        "    # 5. Normalization\n",
        "    row_sum = np.sum(adj, axis=1)\n",
        "    row_sum[row_sum == 0] = 1.0 # Prevent division by zero for isolated nodes\n",
        "    adj_norm = adj / row_sum[:, np.newaxis]\n",
        "\n",
        "    return torch.tensor(adj_norm, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOKmZKBqppYS",
        "outputId": "eaefe636-27d4-471c-856b-a9657976f994"
      },
      "outputs": [],
      "source": [
        "# adj_matrix = get_adjacency_matrix(df_station, sigma=0.06, threshold=0.0)\n",
        "adj_matrix = get_adjacency_matrix_dynamic(df_station)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxS_bVpGzway"
      },
      "outputs": [],
      "source": [
        "torch.save(adj_matrix, \"adj_matrix.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 887
        },
        "id": "sthLduqsqleV",
        "outputId": "ccb35a69-09c1-47d9-edb5-dc56d0d4ee55"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def visualize_adjacency_matrix(adj_tensor, station_labels):\n",
        "    \"\"\"\n",
        "    Plots the normalized adjacency matrix as a heatmap.\n",
        "    \"\"\"\n",
        "    # Convert PyTorch Tensor to Numpy array for plotting\n",
        "    if torch.is_tensor(adj_tensor):\n",
        "        adj_matrix_np = adj_tensor.numpy()\n",
        "    else:\n",
        "        adj_matrix_np = adj_tensor\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Create Heatmap\n",
        "    # annot=True shows the actual weight values in the cells\n",
        "    # cmap=\"viridis\" or \"Blues\" are good color schemes\n",
        "    sns.heatmap(adj_matrix_np,\n",
        "                annot=True,\n",
        "                fmt=\".2f\",\n",
        "                cmap=\"viridis\",\n",
        "                xticklabels=station_labels,\n",
        "                yticklabels=station_labels,\n",
        "                cbar_kws={'label': 'Influence Weight'})\n",
        "\n",
        "    plt.title(\"Spatial Graph Connections (Normalized Adjacency Matrix)\")\n",
        "    plt.xlabel(\"Target District (Influenced By)\")\n",
        "    plt.ylabel(\"Source District (Influencing)\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# --- Run Visualization ---\n",
        "# Use the 'keep_stations' list from your earlier code as labels\n",
        "visualize_adjacency_matrix(adj_matrix, keep_stations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw6a8X35_U_k"
      },
      "source": [
        "1. The Strong Diagonal (The \"Self-Loop\")\n",
        "Observation: The diagonal line (top-left to bottom-right) is bright yellow/green, with values like 0.58, 0.62, 0.53.\n",
        "\n",
        "Why this matters: This tells the model: \"The most important predictor for District 101's pollution in the next hour is District 101's own pollution right now.\" This is physically true and stabilizes the LSTM.\n",
        "\n",
        "2. Distinct Clusters (Geographic Neighborhoods)\n",
        "Observation: Look at the bottom right corner (Districts 119, 120, 121, 122). They form a \"block\" of colors.\n",
        "\n",
        "Why this matters: This indicates these districts are physically close to each other.\n",
        "\n",
        "District 121 is heavily influenced by District 120 (weight 0.21) and itself (weight 0.62).\n",
        "\n",
        "It effectively ignores distant districts like 101 or 102 (weight 0.00).\n",
        "\n",
        "This proves your dynamic sigma calculation successfully identified local neighborhoods.\n",
        "\n",
        "3. True Sparsity (The \"Dark\" Areas)\n",
        "Observation: Large portions of the matrix are dark purple (0.00).\n",
        "\n",
        "Why this matters: In your previous attempt, District 101 was influenced by District 122. In reality, wind doesn't teleport pollution instantly across the whole city.\n",
        "\n",
        "By forcing these weights to zero, you have removed noise. The model will no longer try to find fake patterns between unrelated districts.\n",
        "\n",
        "Conclusion\n",
        "You have successfully moved from a \"Global Average\" model (the first image) to a true Spatio-Temporal Graph (the second image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jYuytK4prJA"
      },
      "outputs": [],
      "source": [
        "class TGCN(nn.Module):\n",
        "    def __init__(self, num_nodes, num_features, hidden_dim, output_dim, adj_matrix):\n",
        "        super(TGCN, self).__init__()\n",
        "        self.num_nodes = num_nodes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.adj = adj_matrix\n",
        "\n",
        "        self.gcn_weight = nn.Parameter(torch.FloatTensor(num_features, hidden_dim))\n",
        "        self.gcn_bias = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
        "        self.gcn_weight.data.uniform_(-stdv, stdv)\n",
        "        self.gcn_bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, num_nodes, _ = x.size()\n",
        "        x_reshaped = x.view(-1, num_nodes, x.size(3))\n",
        "        ax = torch.matmul(self.adj, x_reshaped)\n",
        "        gcn_out = torch.relu(torch.matmul(ax, self.gcn_weight) + self.gcn_bias)\n",
        "        gcn_out = gcn_out.view(batch_size, seq_len, num_nodes, self.hidden_dim)\n",
        "        lstm_input = gcn_out.permute(0, 2, 1, 3).contiguous().view(batch_size * num_nodes, seq_len, self.hidden_dim)\n",
        "        lstm_out, _ = self.lstm(lstm_input)\n",
        "        out = self.fc(lstm_out[:, -1, :])\n",
        "        return out.view(batch_size, num_nodes, -1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ba02tbVHpsVv"
      },
      "outputs": [],
      "source": [
        "model = TGCN(NUM_NODES, NUM_FEATURES, 64, TARGET_DIM, adj_matrix)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-E6cSXqm02M",
        "outputId": "3f604fcd-94fe-4349-dae5-f96ecea9b741"
      },
      "outputs": [],
      "source": [
        "# Check shapes\n",
        "print(f\"Original train_y shape: {train_y.shape}\")\n",
        "\n",
        "# If train_y is (Batch, 6, 13), we must swap the last two dimensions to get (Batch, 13, 6)\n",
        "if train_y.shape[-1] == 13 and train_y.shape[-2] == 6:\n",
        "    print(\"Fixing dimensions...\")\n",
        "    train_y = train_y.permute(0, 2, 1)\n",
        "    train_x = train_x  # x usually doesn't need changing if it's (Batch, Seq, Nodes, Features)\n",
        "\n",
        "print(f\"Corrected train_y shape: {train_y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaxj1oRkmz1J",
        "outputId": "e05a820e-8568-40f0-ad81-72827df8ec7f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create DataLoader for Batching\n",
        "total_samples = len(train_x)\n",
        "split_idx = int(total_samples * 0.8)\n",
        "\n",
        "# Train on the first 80%\n",
        "x_train_tensor = train_x[:split_idx]\n",
        "y_train_tensor = train_y[:split_idx]\n",
        "\n",
        "# Test on the last 20% (Future data)\n",
        "x_test_tensor = train_x[split_idx:]\n",
        "y_test_tensor = train_y[split_idx:]\n",
        "\n",
        "print(f\"Training Samples: {len(x_train_tensor)}\")\n",
        "print(f\"Testing Samples:  {len(x_test_tensor)}\")\n",
        "\n",
        "# Train Loader: Shuffle=True is GOOD here (helps the model learn robustly)\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Test Loader: Shuffle=FALSE is CRITICAL here\n",
        "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEWQLF4jpu2Q",
        "outputId": "2c639e28-20e8-40f4-cda8-81eae00f943a"
      },
      "outputs": [],
      "source": [
        "print(\"Starting Training Loop...\")\n",
        "\n",
        "model.train()\n",
        "for epoch in range(EPOCHS):\n",
        "    total_loss = 0\n",
        "    for batch_x, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        predictions = model(batch_x)\n",
        "\n",
        "        # DEBUG: Print shapes once to verify\n",
        "        if epoch == 0 and total_loss == 0:\n",
        "            print(f\"Predictions shape: {predictions.shape}\") # Should be [32, 13, 6]\n",
        "            print(f\"Targets shape:     {batch_y.shape}\")     # Should be [32, 13, 6]\n",
        "\n",
        "        loss = criterion(predictions, batch_y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "print(\"Training Complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq4S9J7uHCap"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# model is your PyTorch model instance\n",
        "torch.save(model.state_dict(), 'model_weights.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFjsMowIHEey",
        "outputId": "29c8560c-3287-4d8a-aa35-7b9c1e2f3e54"
      },
      "outputs": [],
      "source": [
        "# 1. Recreate the model architecture\n",
        "model = TGCN(NUM_NODES, NUM_FEATURES, 64, TARGET_DIM, adj_matrix)\n",
        "\n",
        "# 2. Load the saved weights\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "# model.eval()  # set to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_pszreGDz20"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "all_preds = []\n",
        "all_targets = []\n",
        "\n",
        "# 1. Collect Predictions\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        preds = model(inputs)\n",
        "        all_preds.append(preds.cpu().numpy())\n",
        "        all_targets.append(targets.cpu().numpy())\n",
        "\n",
        "#new: Shape is now (Total_Samples, 13, 6)\n",
        "predictions = np.concatenate(all_preds, axis=0)\n",
        "actuals = np.concatenate(all_targets, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DNKRx0MiSHa"
      },
      "outputs": [],
      "source": [
        "# Helper function to inverse transform multi-feature output\n",
        "def inverse_transform_multifeature(data_3d, scalers, target_cols):\n",
        "    \"\"\"\n",
        "    data_3d shape: (Time, Nodes, Features)\n",
        "    \"\"\"\n",
        "    samples, nodes, feats = data_3d.shape\n",
        "    output = np.zeros_like(data_3d)\n",
        "\n",
        "    for f_idx, feature_name in enumerate(target_cols):\n",
        "        # 1. Extract the specific feature slice (Time, Nodes)\n",
        "        slice_data = data_3d[:, :, f_idx]\n",
        "\n",
        "        # 2. Reshape to (Time*Nodes, 1) for the scaler\n",
        "        # slice_flat = slice_data.reshape(-1, 1)\n",
        "\n",
        "        # 3. Inverse transform just this feature\n",
        "        # Note: Scaler expects (N, 13) usually if trained that way,\n",
        "        # but here we reshape to fit the scaler's logic.\n",
        "        # IF your scalers were fit on shape (-1, 1), do this:\n",
        "        slice_inv = scalers[feature_name].inverse_transform(slice_data)\n",
        "\n",
        "        # 4. Reshape back and store\n",
        "        output[:, :, f_idx] = slice_inv\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t_DsNg4imZ3",
        "outputId": "63cb1889-cea3-4569-eb1b-bf2d0a38ac79"
      },
      "outputs": [],
      "source": [
        "print(\"Inverse transforming predictions...\")\n",
        "pred_inv = inverse_transform_multifeature(predictions, scalers, TARGET_COLS)\n",
        "actual_inv = inverse_transform_multifeature(actuals, scalers, TARGET_COLS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CkUOAdgNips_",
        "outputId": "575cfa82-d5fb-48b4-81f9-6c70cbba8f82"
      },
      "outputs": [],
      "source": [
        "# --- EXAMPLE: Print Results for Station 101, First Test Sample ---\n",
        "station_idx = 0\n",
        "time_idx = 0\n",
        "\n",
        "print(f\"\\nPrediction for Station {station_idx} at T+1:\")\n",
        "print(f\"{'Feature':<10} | {'Pred':<10} | {'Actual':<10}\")\n",
        "print(\"-\" * 35)\n",
        "\n",
        "for i, feat in enumerate(TARGET_COLS):\n",
        "    p_val = pred_inv[time_idx, station_idx, i]\n",
        "    a_val = actual_inv[time_idx, station_idx, i]\n",
        "    print(f\"{feat:<10} | {p_val:.4f}     | {a_val:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "yREMYfaOELI7",
        "outputId": "91419404-5b55-42b9-ff56-e57ca4bb7fa6"
      },
      "outputs": [],
      "source": [
        " # --- Plot 1: Overall Performance (First District as example) ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(actuals[:200, 0, 0], label='Actual PM2.5 (District 0)', color='black', alpha=0.7)\n",
        "plt.plot(predictions[:200, 0, 0], label='Predicted PM2.5', color='cyan', linestyle='--')\n",
        "plt.title(\"Forecasting: District 0 (First 200 Hours)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7vv5DhkzGsip",
        "outputId": "792a85e5-4fa2-4e65-9828-eaf5fecf866f"
      },
      "outputs": [],
      "source": [
        "# --- Plot 2: Error Distribution by District ---\n",
        "# Calculate MAE for each district separately\n",
        "mae_per_district = np.mean(np.abs(predictions - actuals), axis=0).flatten()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=list(range(13)), y=mae_per_district, palette=\"viridis\")\n",
        "plt.xlabel(\"District ID\")\n",
        "plt.ylabel(\"Mean Absolute Error (PM2.5)\")\n",
        "plt.title(\"Model Error per District\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSyx_f-x0xnj"
      },
      "source": [
        "manual input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8W4D_y60yrX"
      },
      "outputs": [],
      "source": [
        "def create_template_csv():\n",
        "    dates = pd.date_range(end='2023-12-05 13:00', periods=24, freq='H')\n",
        "    stations = [101, 102, 105, 106, 107, 109, 111, 112, 113, 119, 120, 121, 122]\n",
        "\n",
        "    rows = []\n",
        "    for d in dates:\n",
        "        for s in stations:\n",
        "            rows.append({\n",
        "                'Measurement date': d,\n",
        "                'Station code': s,\n",
        "                'SO2': 0, 'NO2': 0, 'O3': 0, 'CO': 0, 'PM10': 0, 'PM2.5': 0\n",
        "            })\n",
        "\n",
        "    pd.DataFrame(rows).to_csv('prediction_input_template.csv', index=False)\n",
        "    print(\"Template 'prediction_input_template.csv' created. Fill it with data.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tq_-gRSY06sP",
        "outputId": "69659a7c-049d-44e6-fffc-60e565b7c838"
      },
      "outputs": [],
      "source": [
        "create_template_csv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pzTNHCyboIjn"
      },
      "outputs": [],
      "source": [
        "adj_matrix = torch.load(\"/content/adj_matrix.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdZc1mlLoET0",
        "outputId": "a70a5059-0de7-4f72-c2ec-46b3c1c17b5e"
      },
      "outputs": [],
      "source": [
        "# 1. Recreate the model architecture\n",
        "model = TGCN(NUM_NODES, NUM_FEATURES, 64, TARGET_DIM, adj_matrix)\n",
        "\n",
        "# 2. Load the saved weights\n",
        "model.load_state_dict(torch.load('/content/model_weights.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjt2PgfQoNg3"
      },
      "outputs": [],
      "source": [
        "import joblib\n",
        "scalers = joblib.load(\"/content/seoul_scalers.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJSiSJU80-kf"
      },
      "outputs": [],
      "source": [
        "def predict_from_csv(csv_path, model, scalers):\n",
        "    \"\"\"\n",
        "    Reads a CSV, processes the last 24 hours of data, and forecasts PM2.5.\n",
        "    \"\"\"\n",
        "    # 1. Load Data\n",
        "    input_df = pd.read_csv(csv_path)\n",
        "\n",
        "    # 2. Preprocessing (Must match training logic exactly)\n",
        "    input_df['Measurement date'] = pd.to_datetime(input_df['Measurement date'])\n",
        "\n",
        "    # Filter for the 13 specific stations used in training\n",
        "    keep_stations = [101, 102, 105, 106, 107, 109, 111, 112, 113, 119, 120, 121, 122]\n",
        "    input_df = input_df[input_df['Station code'].isin(keep_stations)]\n",
        "\n",
        "    # Sort to ensure node alignment (Time -> Station)\n",
        "    input_df = input_df.sort_values(by=['Measurement date', 'Station code'])\n",
        "\n",
        "    # Time Encoding\n",
        "    input_df['hour'] = input_df['Measurement date'].dt.hour\n",
        "    input_df['month'] = input_df['Measurement date'].dt.month\n",
        "    input_df['hour_sin'] = np.sin(2 * np.pi * input_df['hour'] / 24.0)\n",
        "    input_df['hour_cos'] = np.cos(2 * np.pi * input_df['hour'] / 24.0)\n",
        "    input_df['month_sin'] = np.sin(2 * np.pi * input_df['month'] / 12.0)\n",
        "    input_df['month_cos'] = np.cos(2 * np.pi * input_df['month'] / 12.0)\n",
        "\n",
        "    # 3. Check Data Sufficiency\n",
        "    # We need exactly 24 hours for 13 stations = 312 rows\n",
        "    # We take the LAST 24 hours available in the CSV\n",
        "    timestamps = input_df['Measurement date'].unique()\n",
        "    if len(timestamps) < 24:\n",
        "        raise ValueError(f\"CSV only has {len(timestamps)} hours of data. Model requires 24 hours history.\")\n",
        "\n",
        "    last_24_hours = timestamps[-24:]\n",
        "    input_df = input_df[input_df['Measurement date'].isin(last_24_hours)]\n",
        "\n",
        "    # 4. Prepare Features (Normalize using TRAINED scalers)\n",
        "    feature_cols = ['NO2', 'O3', 'CO', 'SO2', 'PM10', 'PM2.5',\n",
        "                    'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\n",
        "\n",
        "    processed_features = []\n",
        "\n",
        "    for feat in feature_cols:\n",
        "        # Reshape to (24 hours, 13 stations)\n",
        "        values = input_df[feat].values.reshape(24, 13)\n",
        "\n",
        "        # Handle Missing Values (Interpolate just like training)\n",
        "        val_df = pd.DataFrame(values)\n",
        "        val_df = val_df.interpolate(method='linear', limit_direction='both').bfill().ffill()\n",
        "\n",
        "        # Scale\n",
        "        if 'sin' not in feat and 'cos' not in feat:\n",
        "            # CRITICAL: Use .transform(), do NOT use .fit_transform()\n",
        "            if feat in scalers:\n",
        "                values_norm = scalers[feat].transform(val_df.values)\n",
        "            else:\n",
        "                print(f\"Warning: Scaler for {feat} not found. Using raw values.\")\n",
        "                values_norm = val_df.values\n",
        "        else:\n",
        "            values_norm = val_df.values\n",
        "\n",
        "        processed_features.append(values_norm)\n",
        "\n",
        "    # Stack to get shape (24, 13, 10)\n",
        "    input_seq = np.stack(processed_features, axis=-1)\n",
        "\n",
        "    # Add Batch Dimension: (1, 24, 13, 10)\n",
        "    input_tensor = torch.FloatTensor(input_seq).unsqueeze(0)\n",
        "\n",
        "    # 5. Prediction\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Output shape: (1, 13, 6)\n",
        "        prediction = model(input_tensor)\n",
        "\n",
        "    # 6. Inverse Scale PM2.5\n",
        "    # PM2.5 is at index 5 in the target list\n",
        "    # prediction shape is (1, 13, 6) -> We want (13,) values for PM2.5\n",
        "    pm25_values = prediction[0, :, 5].numpy()\n",
        "\n",
        "    # CRITICAL FIX: Reshape to (1, 13)\n",
        "    # The scaler expects (n_samples, 13_stations), so we pretend this is 1 sample of 13 stations\n",
        "    pm25_input_for_scaler = pm25_values.reshape(1, 13)\n",
        "\n",
        "    # Inverse transform\n",
        "    pm25_actual = scalers['PM2.5'].inverse_transform(pm25_input_for_scaler)\n",
        "\n",
        "    # Flatten back to a list of 13 values for the dataframe\n",
        "    pm25_final = pm25_actual.flatten()\n",
        "\n",
        "    # 7. Format Results\n",
        "    results = pd.DataFrame({\n",
        "        'Station Code': keep_stations,\n",
        "        'Predicted PM2.5': pm25_final\n",
        "    })\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkC_AqMx02G_",
        "outputId": "3622d45e-9495-4b72-8888-0e0d65ca2323"
      },
      "outputs": [],
      "source": [
        "\n",
        "# --- Usage Example ---\n",
        "# Assuming 'model' is trained and 'scalers' is populated from your previous run\n",
        "try:\n",
        "    # Use a CSV that has at least the last 24 hours of data\n",
        "    forecast_df = predict_from_csv('/content/aqi_data.csv', model, scalers)\n",
        "\n",
        "    print(\"\\n--- Forecast for Next Hour ---\")\n",
        "    print(forecast_df)\n",
        "\n",
        "    # Optional: Save to CSV\n",
        "    # forecast_df.to_csv('forecast_results.csv', index=False)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during prediction: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWrv9AmmD6v0"
      },
      "source": [
        "Prediction for hour following: 2023-12-05 13:00:00 means it forecasts the value for the next hour (which is 14:00), since our model was trained to predict the next hour.\n",
        "\n",
        "can change the code into\n",
        "```\n",
        "# Calculate the actual future time\n",
        "future_time = pd.to_datetime(time_point) + pd.Timedelta(hours=1)\n",
        "print(f\"Prediction target time: {future_time}\")\n",
        "```\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
