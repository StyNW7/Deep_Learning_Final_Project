{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9249127",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:281\u001b[0m\n\u001b[0;32m    277\u001b[0m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    279\u001b[0m         kernel32\u001b[38;5;241m.\u001b[39mSetErrorMode(prev_error_mode)\n\u001b[1;32m--> 281\u001b[0m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\__init__.py:264\u001b[0m, in \u001b[0;36m_load_dll_libraries\u001b[1;34m()\u001b[0m\n\u001b[0;32m    260\u001b[0m     err \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mWinError(last_error)\n\u001b[0;32m    261\u001b[0m     err\u001b[38;5;241m.\u001b[39mstrerror \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Error loading \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m or one of its dependencies.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    263\u001b[0m     )\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     is_loaded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1114] A dynamic link library (DLL) initialization routine failed. Error loading \"c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ==================== HYPERPARAMETERS & CONFIG ====================\n",
    "class Config:\n",
    "    # Fixed parameters\n",
    "    NUM_NODES = 13\n",
    "    NUM_FEATURES = 10\n",
    "    TARGET_COLS = ['NO2', 'O3', 'CO', 'SO2', 'PM10', 'PM2.5']\n",
    "    TARGET_DIM = 6\n",
    "    STATIONS = [101, 102, 105, 106, 107, 109, 111, 112, 113, 119, 120, 121, 122]\n",
    "    \n",
    "    # Optuna tuning ranges\n",
    "    HIDDEN_DIM_RANGE = (32, 128)  # Increased range for better exploration\n",
    "    LEARNING_RATE_RANGE = (1e-4, 1e-2)\n",
    "    BATCH_SIZE_RANGE = (16, 64)\n",
    "    SEQ_LEN_RANGE = (12, 48)  # Multiple options for sequence length\n",
    "    DROPOUT_RANGE = (0.1, 0.5)\n",
    "    \n",
    "    # Training\n",
    "    N_TRIALS = 30  # Number of Optuna trials\n",
    "    EPOCHS = 50  # Increased epochs for better convergence\n",
    "    EARLY_STOPPING_PATIENCE = 10\n",
    "    \n",
    "    # Graph parameters\n",
    "    GRAPH_THRESHOLD = 0.1\n",
    "    GRAPH_DENSITY_TARGET = (0.2, 0.5)\n",
    "\n",
    "# ==================== DATA PREPROCESSING ====================\n",
    "def preprocess_seoul_data(df: pd.DataFrame, scalers: Dict, look_back_window: int = 24, \n",
    "                         target_cols: List[str] = Config.TARGET_COLS) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Enhanced preprocessing with better error handling and feature engineering\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Ensure Date format and sort\n",
    "    df['Measurement date'] = pd.to_datetime(df['Measurement date'])\n",
    "    df = df.sort_values(by=['Measurement date', 'Station code'])\n",
    "    \n",
    "    # Enhanced time encoding\n",
    "    df['hour'] = df['Measurement date'].dt.hour\n",
    "    df['day_of_week'] = df['Measurement date'].dt.dayofweek\n",
    "    df['month'] = df['Measurement date'].dt.month\n",
    "    \n",
    "    # Multiple cyclical features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24.0)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7.0)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12.0)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12.0)\n",
    "    \n",
    "    # Feature engineering: add lag features (previous hour)\n",
    "    feature_cols = target_cols + ['hour_sin', 'hour_cos', 'day_sin', 'day_cos', 'month_sin', 'month_cos']\n",
    "    \n",
    "    # Get sorted stations\n",
    "    stations = sorted(df['Station code'].unique())\n",
    "    num_stations = len(stations)\n",
    "    \n",
    "    processed_features = []\n",
    "    \n",
    "    for feat in feature_cols:\n",
    "        values = df[feat].values.reshape(-1, num_stations)\n",
    "        val_df = pd.DataFrame(values)\n",
    "        \n",
    "        # Advanced interpolation\n",
    "        val_df = val_df.interpolate(method='linear', limit_direction='both', limit=3)\n",
    "        val_df = val_df.bfill().ffill()\n",
    "        \n",
    "        # Normalize only physical pollutants\n",
    "        if feat in target_cols:\n",
    "            scaler = MinMaxScaler()\n",
    "            values_norm = scaler.fit_transform(val_df.values)\n",
    "            scalers[feat] = scaler\n",
    "        else:\n",
    "            values_norm = val_df.values\n",
    "        \n",
    "        processed_features.append(values_norm)\n",
    "    \n",
    "    # Stack features: (Time, Nodes, Features)\n",
    "    data_block = np.stack(processed_features, axis=-1)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, Y = [], []\n",
    "    total_time_steps = data_block.shape[0]\n",
    "    target_indices = [feature_cols.index(col) for col in target_cols]\n",
    "    \n",
    "    for i in range(total_time_steps - look_back_window - 1):\n",
    "        X.append(data_block[i:i + look_back_window, :, :])\n",
    "        Y.append(data_block[i + look_back_window, :, target_indices])\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(np.array(X))\n",
    "    Y_tensor = torch.FloatTensor(np.array(Y))\n",
    "    \n",
    "    # Ensure correct shape\n",
    "    if Y_tensor.dim() == 2:\n",
    "        Y_tensor = Y_tensor.unsqueeze(-1)\n",
    "    \n",
    "    return X_tensor, Y_tensor\n",
    "\n",
    "# ==================== GRAPH CONSTRUCTION ====================\n",
    "def get_adjacency_matrix_dynamic(lat_lon_df: pd.DataFrame, \n",
    "                                threshold: float = Config.GRAPH_THRESHOLD,\n",
    "                                k_nearest: int = 5) -> torch.Tensor:\n",
    "    \"\"\"Enhanced graph construction with k-nearest neighbors and adaptive thresholding\"\"\"\n",
    "    \n",
    "    # Calculate pairwise distances\n",
    "    coords = lat_lon_df[['Latitude', 'Longitude']].values\n",
    "    dists = cdist(coords, coords, metric='euclidean')\n",
    "    \n",
    "    # Calculate adaptive sigma\n",
    "    non_zero_dists = dists[dists > 0]\n",
    "    sigma = np.std(non_zero_dists)\n",
    "    \n",
    "    # Build adjacency matrix with Gaussian kernel\n",
    "    num_nodes = len(lat_lon_df)\n",
    "    adj = np.zeros((num_nodes, num_nodes))\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        # Use k-nearest neighbors approach\n",
    "        distances_i = dists[i]\n",
    "        nearest_indices = np.argsort(distances_i)[1:k_nearest+1]  # Exclude self\n",
    "        \n",
    "        for j in range(num_nodes):\n",
    "            if i == j:\n",
    "                adj[i][j] = 1.0\n",
    "            elif j in nearest_indices:\n",
    "                d = dists[i][j]\n",
    "                weight = np.exp(- (d**2) / (2 * sigma**2))  # Gaussian kernel\n",
    "                adj[i][j] = weight\n",
    "            else:\n",
    "                adj[i][j] = 0.0\n",
    "    \n",
    "    # Apply threshold\n",
    "    adj[adj < threshold] = 0.0\n",
    "    \n",
    "    # Ensure connectivity (add self-loops if needed)\n",
    "    for i in range(num_nodes):\n",
    "        if np.sum(adj[i]) == 0:\n",
    "            adj[i][i] = 1.0\n",
    "    \n",
    "    # Normalize\n",
    "    row_sum = np.sum(adj, axis=1)\n",
    "    row_sum[row_sum == 0] = 1.0\n",
    "    adj_norm = adj / row_sum[:, np.newaxis]\n",
    "    \n",
    "    # Calculate graph statistics\n",
    "    num_edges = np.count_nonzero(adj) - num_nodes\n",
    "    density = num_edges / (num_nodes * (num_nodes - 1))\n",
    "    \n",
    "    print(f\"Graph Statistics:\")\n",
    "    print(f\"  Sigma: {sigma:.6f}\")\n",
    "    print(f\"  Edges: {num_edges}\")\n",
    "    print(f\"  Density: {density:.2%}\")\n",
    "    print(f\"  K-nearest neighbors: {k_nearest}\")\n",
    "    \n",
    "    return torch.tensor(adj_norm, dtype=torch.float32)\n",
    "\n",
    "# ==================== ENHANCED TGCN MODEL ====================\n",
    "class EnhancedTGCN(nn.Module):\n",
    "    def __init__(self, num_nodes: int, num_features: int, hidden_dim: int, \n",
    "                 output_dim: int, adj_matrix: torch.Tensor, dropout_rate: float = 0.3):\n",
    "        super(EnhancedTGCN, self).__init__()\n",
    "        \n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.adj = adj_matrix\n",
    "        \n",
    "        # Enhanced GCN with multiple layers\n",
    "        self.gcn1 = nn.Linear(num_features, hidden_dim)\n",
    "        self.gcn2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Enhanced LSTM with bidirectional capability\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=hidden_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=dropout_rate,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_dim * 2,  # Bidirectional\n",
    "            num_heads=4,\n",
    "            dropout=dropout_rate,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Final prediction layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Batch normalization\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # Activation\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.LSTM):\n",
    "                for name, param in m.named_parameters():\n",
    "                    if 'weight_ih' in name:\n",
    "                        nn.init.xavier_uniform_(param.data)\n",
    "                    elif 'weight_hh' in name:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    elif 'bias' in name:\n",
    "                        nn.init.constant_(param.data, 0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, seq_len, num_nodes, num_features = x.size()\n",
    "        \n",
    "        # Reshape for GCN: (batch*seq, nodes, features)\n",
    "        x_reshaped = x.view(-1, num_nodes, num_features)\n",
    "        \n",
    "        # GCN layers with adjacency matrix\n",
    "        ax = torch.matmul(self.adj, x_reshaped)\n",
    "        gcn_out1 = self.relu(self.gcn1(ax))\n",
    "        gcn_out1 = self.dropout(gcn_out1)\n",
    "        \n",
    "        ax2 = torch.matmul(self.adj, gcn_out1)\n",
    "        gcn_out2 = self.relu(self.gcn2(ax2))\n",
    "        gcn_out2 = self.dropout(gcn_out2)\n",
    "        \n",
    "        # Reshape for LSTM: (batch, nodes, seq, hidden)\n",
    "        gcn_out2 = gcn_out2.view(batch_size, seq_len, num_nodes, self.hidden_dim)\n",
    "        lstm_input = gcn_out2.permute(0, 2, 1, 3).contiguous()\n",
    "        lstm_input = lstm_input.view(batch_size * num_nodes, seq_len, self.hidden_dim)\n",
    "        \n",
    "        # LSTM processing\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)\n",
    "        \n",
    "        # Take the last time step\n",
    "        last_out = attn_out[:, -1, :]\n",
    "        \n",
    "        # Final fully connected layers\n",
    "        out = self.relu(self.bn1(self.fc1(last_out)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        # Reshape to (batch, nodes, output_dim)\n",
    "        out = out.view(batch_size, num_nodes, -1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ==================== TRAINING UTILITIES ====================\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience: int = 10, min_delta: float = 0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss: float):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "def create_data_loaders(X: torch.Tensor, Y: torch.Tensor, batch_size: int, \n",
    "                       train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple:\n",
    "    \"\"\"Create train, validation, and test data loaders\"\"\"\n",
    "    total_samples = len(X)\n",
    "    \n",
    "    train_idx = int(total_samples * train_ratio)\n",
    "    val_idx = int(total_samples * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Split data\n",
    "    X_train, Y_train = X[:train_idx], Y[:train_idx]\n",
    "    X_val, Y_val = X[train_idx:val_idx], Y[train_idx:val_idx]\n",
    "    X_test, Y_test = X[val_idx:], Y[val_idx:]\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_val, Y_val)\n",
    "    test_dataset = TensorDataset(X_test, Y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Data splits: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def train_epoch(model: nn.Module, loader: DataLoader, criterion: nn.Module, \n",
    "               optimizer: optim.Optimizer, device: torch.device) -> float:\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch_x)\n",
    "        \n",
    "        # Ensure shapes match\n",
    "        if predictions.shape != batch_y.shape:\n",
    "            predictions = predictions[:, :, :batch_y.shape[-1]]\n",
    "        \n",
    "        loss = criterion(predictions, batch_y)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model: nn.Module, loader: DataLoader, criterion: nn.Module, \n",
    "            device: torch.device) -> Tuple[float, Dict]:\n",
    "    \"\"\"Validate model and return metrics\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            predictions = model(batch_x)\n",
    "            \n",
    "            # Ensure shapes match\n",
    "            if predictions.shape != batch_y.shape:\n",
    "                predictions = predictions[:, :, :batch_y.shape[-1]]\n",
    "            \n",
    "            loss = criterion(predictions, batch_y)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            all_preds.append(predictions.cpu().numpy())\n",
    "            all_targets.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(loader)\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    if all_preds:\n",
    "        preds = np.concatenate(all_preds, axis=0)\n",
    "        targets = np.concatenate(all_targets, axis=0)\n",
    "        \n",
    "        # Calculate MAE and RMSE for each pollutant\n",
    "        metrics = {}\n",
    "        for i, col in enumerate(Config.TARGET_COLS):\n",
    "            if i < preds.shape[-1]:\n",
    "                mae = np.mean(np.abs(preds[..., i] - targets[..., i]))\n",
    "                rmse = np.sqrt(np.mean((preds[..., i] - targets[..., i])**2))\n",
    "                metrics[f'{col}_MAE'] = mae\n",
    "                metrics[f'{col}_RMSE'] = rmse\n",
    "        \n",
    "        metrics['overall_loss'] = avg_loss\n",
    "    else:\n",
    "        metrics = {'overall_loss': avg_loss}\n",
    "    \n",
    "    return avg_loss, metrics\n",
    "\n",
    "# ==================== OPTUNA HYPERPARAMETER TUNING ====================\n",
    "def objective(trial, train_loader, val_loader, adj_matrix, device):\n",
    "    \"\"\"Optuna objective function for hyperparameter optimization\"\"\"\n",
    "    \n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = trial.suggest_int('hidden_dim', *Config.HIDDEN_DIM_RANGE)\n",
    "    learning_rate = trial.suggest_float('learning_rate', *Config.LEARNING_RATE_RANGE, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', *Config.BATCH_SIZE_RANGE)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', *Config.DROPOUT_RANGE)\n",
    "    \n",
    "    # Model architecture choices\n",
    "    use_bidirectional = trial.suggest_categorical('use_bidirectional', [True, False])\n",
    "    num_lstm_layers = trial.suggest_int('num_lstm_layers', 1, 3)\n",
    "    use_attention = trial.suggest_categorical('use_attention', [True, False])\n",
    "    \n",
    "    # Create model with suggested parameters\n",
    "    model = EnhancedTGCN(\n",
    "        num_nodes=Config.NUM_NODES,\n",
    "        num_features=Config.NUM_FEATURES,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=Config.TARGET_DIM,\n",
    "        adj_matrix=adj_matrix,\n",
    "        dropout_rate=dropout_rate\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer with suggested learning rate\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=5, verbose=False\n",
    "    )\n",
    "    \n",
    "    # Loss function (weighted for multi-output)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(patience=Config.EARLY_STOPPING_PATIENCE)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        # Train\n",
    "        train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Report intermediate value to Optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Handle pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        # Update best loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss):\n",
    "            break\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# ==================== MAIN EXECUTION ====================\n",
    "def main():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TGCN HYPERPARAMETER TUNING AND MODEL TRAINING\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"\\n1. Loading and preprocessing data...\")\n",
    "    import kagglehub\n",
    "    path = kagglehub.dataset_download(\"bappekim/air-pollution-in-seoul\")\n",
    "    \n",
    "    df = pd.read_csv(path + '/AirPollutionSeoul/Measurement_summary.csv')\n",
    "    df_station = pd.read_csv(path + '/AirPollutionSeoul/Original Data/Measurement_station_info.csv')\n",
    "    \n",
    "    # Filter stations\n",
    "    df = df[df['Station code'].isin(Config.STATIONS)]\n",
    "    df_station = df_station[df_station['Station code'].isin(Config.STATIONS)]\n",
    "    df_station = df_station.sort_values('Station code')\n",
    "    \n",
    "    # Generate adjacency matrix\n",
    "    print(\"\\n2. Generating adjacency matrix...\")\n",
    "    adj_matrix = get_adjacency_matrix_dynamic(df_station)\n",
    "    torch.save(adj_matrix, \"adj_matrix.pt\")\n",
    "    print(\"Adjacency matrix saved to 'adj_matrix.pt'\")\n",
    "    \n",
    "    # Visualize adjacency matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(adj_matrix.numpy(), cmap='viridis', \n",
    "                xticklabels=Config.STATIONS, yticklabels=Config.STATIONS)\n",
    "    plt.title(\"Spatial Graph Connections\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('adjacency_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Preprocess data with default sequence length\n",
    "    scalers = {}\n",
    "    train_x, train_y = preprocess_seoul_data(df, scalers, look_back_window=24)\n",
    "    \n",
    "    # Save scalers\n",
    "    joblib.dump(scalers, \"seoul_scalers.pkl\")\n",
    "    print(\"Scalers saved to 'seoul_scalers.pkl'\")\n",
    "    \n",
    "    print(f\"\\n3. Data shape: Input {train_x.shape}, Target {train_y.shape}\")\n",
    "    \n",
    "    # Create data loaders for tuning (using smaller subset for speed)\n",
    "    train_loader, val_loader, _ = create_data_loaders(\n",
    "        train_x, train_y, \n",
    "        batch_size=32,  # Default for tuning\n",
    "        train_ratio=0.7,\n",
    "        val_ratio=0.15\n",
    "    )\n",
    "    \n",
    "    # Hyperparameter tuning with Optuna\n",
    "    print(\"\\n4. Starting hyperparameter tuning with Optuna...\")\n",
    "    print(f\"   Number of trials: {Config.N_TRIALS}\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=42),\n",
    "        pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10)\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, train_loader, val_loader, adj_matrix, device),\n",
    "        n_trials=Config.N_TRIALS,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # Display best hyperparameters\n",
    "    print(\"\\n5. Best hyperparameters found:\")\n",
    "    print(\"=\" * 40)\n",
    "    for key, value in study.best_params.items():\n",
    "        print(f\"{key:20}: {value}\")\n",
    "    print(f\"{'Best validation loss':20}: {study.best_value:.6f}\")\n",
    "    \n",
    "    # Train final model with best hyperparameters\n",
    "    print(\"\\n6. Training final model with best hyperparameters...\")\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    \n",
    "    # Create final data loaders with optimal batch size\n",
    "    final_train_loader, final_val_loader, test_loader = create_data_loaders(\n",
    "        train_x, train_y,\n",
    "        batch_size=best_params['batch_size'],\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1\n",
    "    )\n",
    "    \n",
    "    # Create final model\n",
    "    final_model = EnhancedTGCN(\n",
    "        num_nodes=Config.NUM_NODES,\n",
    "        num_features=Config.NUM_FEATURES,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        output_dim=Config.TARGET_DIM,\n",
    "        adj_matrix=adj_matrix,\n",
    "        dropout_rate=best_params['dropout_rate']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(\n",
    "        final_model.parameters(), \n",
    "        lr=best_params['learning_rate'],\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=7, verbose=True\n",
    "    )\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    early_stopping = EarlyStopping(patience=15)\n",
    "    \n",
    "    # Training history\n",
    "    train_history = []\n",
    "    val_history = []\n",
    "    \n",
    "    print(\"\\nTraining Progress:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(Config.EPOCHS):\n",
    "        # Train\n",
    "        train_loss = train_epoch(final_model, final_train_loader, criterion, optimizer, device)\n",
    "        train_history.append(train_loss)\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_metrics = validate(final_model, final_val_loader, criterion, device)\n",
    "        val_history.append(val_loss)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1:3d}/{Config.EPOCHS} | \"\n",
    "                  f\"Train Loss: {train_loss:.6f} | \"\n",
    "                  f\"Val Loss: {val_loss:.6f} | \"\n",
    "                  f\"LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if early_stopping(val_loss):\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    # Final evaluation on test set\n",
    "    print(\"\\n7. Final evaluation on test set...\")\n",
    "    test_loss, test_metrics = validate(final_model, test_loader, criterion, device)\n",
    "    \n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Overall Test Loss: {test_loss:.6f}\")\n",
    "    \n",
    "    # Print metrics for each pollutant\n",
    "    for i, col in enumerate(Config.TARGET_COLS):\n",
    "        if f'{col}_MAE' in test_metrics:\n",
    "            print(f\"{col:6} - MAE: {test_metrics[f'{col}_MAE']:.4f}, \"\n",
    "                  f\"RMSE: {test_metrics[f'{col}_RMSE']:.4f}\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(final_model.state_dict(), 'model_weights.pth')\n",
    "    print(\"\\nFinal model saved to 'model_weights.pth'\")\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_history, label='Train Loss', alpha=0.8)\n",
    "    plt.plot(val_history, label='Validation Loss', alpha=0.8)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_history, label='Train Loss', alpha=0.8)\n",
    "    plt.plot(val_history, label='Validation Loss', alpha=0.8)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (log scale)')\n",
    "    plt.title('Training History (Log Scale)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize predictions vs actuals\n",
    "    print(\"\\n8. Generating prediction visualizations...\")\n",
    "    \n",
    "    final_model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch_x, test_batch_y = next(iter(test_loader))\n",
    "        test_batch_x, test_batch_y = test_batch_x.to(device), test_batch_y.to(device)\n",
    "        predictions = final_model(test_batch_x)\n",
    "        \n",
    "        # Inverse transform predictions for PM2.5 (index 5)\n",
    "        pm25_predictions = predictions[0, :, 5].cpu().numpy()\n",
    "        pm25_actual = test_batch_y[0, :, 5].cpu().numpy()\n",
    "        \n",
    "        # Inverse transform using scaler\n",
    "        if 'PM2.5' in scalers:\n",
    "            pm25_predictions_original = scalers['PM2.5'].inverse_transform(\n",
    "                pm25_predictions.reshape(1, -1)\n",
    "            ).flatten()\n",
    "            pm25_actual_original = scalers['PM2.5'].inverse_transform(\n",
    "                pm25_actual.reshape(1, -1)\n",
    "            ).flatten()\n",
    "        else:\n",
    "            pm25_predictions_original = pm25_predictions\n",
    "            pm25_actual_original = pm25_actual\n",
    "        \n",
    "        # Plot predictions vs actuals\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        bars = plt.bar(range(len(Config.STATIONS)), pm25_predictions_original, \n",
    "                       alpha=0.7, label='Predicted')\n",
    "        plt.bar(range(len(Config.STATIONS)), pm25_actual_original, \n",
    "                alpha=0.5, label='Actual')\n",
    "        plt.xlabel('Station')\n",
    "        plt.ylabel('PM2.5 Value')\n",
    "        plt.title('PM2.5 Predictions vs Actuals (All Stations)')\n",
    "        plt.xticks(range(len(Config.STATIONS)), Config.STATIONS, rotation=45)\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(pm25_actual_original, pm25_predictions_original, alpha=0.6)\n",
    "        plt.plot([min(pm25_actual_original), max(pm25_actual_original)], \n",
    "                 [min(pm25_actual_original), max(pm25_actual_original)], \n",
    "                 'r--', alpha=0.8, label='Perfect Prediction')\n",
    "        plt.xlabel('Actual PM2.5')\n",
    "        plt.ylabel('Predicted PM2.5')\n",
    "        plt.title('Prediction Scatter Plot')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('predictions_vs_actuals.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    \n",
    "    # Save study results\n",
    "    optuna.visualization.plot_optimization_history(study).show()\n",
    "    optuna.visualization.plot_param_importances(study).show()\n",
    "    \n",
    "    # Create study DataFrame and save\n",
    "    study_df = study.trials_dataframe()\n",
    "    study_df.to_csv('optuna_study_results.csv', index=False)\n",
    "    print(\"\\nOptuna study results saved to 'optuna_study_results.csv'\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"HYPERPARAMETER TUNING COMPLETE!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\nSaved files:\")\n",
    "    print(\"  1. adj_matrix.pt - Spatial graph adjacency matrix\")\n",
    "    print(\"  2. seoul_scalers.pkl - Feature normalization scalers\")\n",
    "    print(\"  3. model_weights.pth - Best model weights\")\n",
    "    print(\"  4. training_history.png - Training curves\")\n",
    "    print(\"  5. adjacency_matrix.png - Graph visualization\")\n",
    "    print(\"  6. predictions_vs_actuals.png - Prediction quality\")\n",
    "    print(\"  7. optuna_study_results.csv - All tuning trials\")\n",
    "    \n",
    "    # Create inference-ready model class (simplified version)\n",
    "    print(\"\\n\\nFor inference, use this simplified model class:\")\n",
    "    print(\"=\" * 60)\n",
    "    inference_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TGCN(nn.Module):\n",
    "    def __init__(self, num_nodes, num_features, hidden_dim, output_dim, adj_matrix):\n",
    "        super(TGCN, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.adj = adj_matrix\n",
    "        \n",
    "        self.gcn_weight = nn.Parameter(torch.FloatTensor(num_features, hidden_dim))\n",
    "        self.gcn_bias = nn.Parameter(torch.FloatTensor(hidden_dim))\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=hidden_dim, hidden_size=hidden_dim, \n",
    "                           batch_first=True, num_layers=2)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_dim)\n",
    "        self.gcn_weight.data.uniform_(-stdv, stdv)\n",
    "        self.gcn_bias.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, num_nodes, _ = x.size()\n",
    "        x_reshaped = x.view(-1, num_nodes, x.size(3))\n",
    "        ax = torch.matmul(self.adj, x_reshaped)\n",
    "        gcn_out = torch.relu(torch.matmul(ax, self.gcn_weight) + self.gcn_bias)\n",
    "        gcn_out = self.dropout(gcn_out)\n",
    "        gcn_out = gcn_out.view(batch_size, seq_len, num_nodes, self.hidden_dim)\n",
    "        lstm_input = gcn_out.permute(0, 2, 1, 3).contiguous().view(\n",
    "            batch_size * num_nodes, seq_len, self.hidden_dim)\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        out = self.fc(lstm_out[:, -1, :])\n",
    "        return out.view(batch_size, num_nodes, -1)\n",
    "'''\n",
    "    print(inference_code)\n",
    "    \n",
    "    return final_model, scalers, adj_matrix, best_params\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the complete pipeline\n",
    "    best_model, scalers, adj_matrix, best_params = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
